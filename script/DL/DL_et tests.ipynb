{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b32b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Prerequisites:\n",
    "# ---------------------------\n",
    "%pip install -r requirements.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Importation des bibliothèques\n",
    "# ---------------------------\n",
    "\n",
    "import pandas as pd                         # Manipulation de données tabulaires (CSV, DataFrame…)\n",
    "import torch                                # Bibliothèque PyTorch pour deep learning\n",
    "import torch.nn as nn                       # Couches de réseaux neuronaux\n",
    "import torch.nn.functional as F             # Fonctions utiles (activations, pertes, normalisation…)\n",
    "import random                               # Générateur aléatoire\n",
    "import math                                 # Fonctions mathématiques (log, ceil…)\n",
    "import dotenv                               # Gestion des variables d'environnement\n",
    "import os                                   # Gestion des fichiers et dossiers\n",
    "from dotenv import load_dotenv,find_dotenv  # Charger variables d'environnement depuis .env\n",
    "from supabase import create_client, Client  # Client Supabase pour Python\n",
    "import boto3                                # Client AWS S3 pour Python\n",
    "import io                                   # Gestion des flux d'entrée/sortie                         \n",
    "import json                                 # Manipulation de données JSON\n",
    "import gcsfs                                # Système de fichiers pour Google Cloud Storage\n",
    "print(\"Bibliothèques importées\")\n",
    "\n",
    "# Localiser et recharger le .env\n",
    "dotenv_path = find_dotenv()  # trouve le .env dans ton projet\n",
    "load_dotenv(dotenv_path, override=True)  # override=True force le remplacement\n",
    "\n",
    "# Vérification des variables d'environnement\n",
    "SUPABASE_BUCKET = os.getenv(\"SUPABASE_BUCKET\") \n",
    "print(\"Bucket utilisé :\", SUPABASE_BUCKET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12996495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Connexion à Supabase\n",
    "# ---------------------------\n",
    "\n",
    "#Configuration Supabase\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\") \n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\") \n",
    "SUPABASE_BUCKET = os.getenv(\"SUPABASE_BUCKET\") \n",
    "file_name = os.getenv(\"file_name\") \n",
    "\n",
    "# Connexion à Supabase\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY) \n",
    "print(\"Connecté à Supabase\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a50450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Charger et lire le CSV \"df_competence_rome_eda_v2\" depuis Supabase Storage\n",
    "# ---------------------------\n",
    "\n",
    "def read_csv_from_supabase(file_name, bucket_name=SUPABASE_BUCKET): \n",
    "    try:\n",
    "        # Télécharger le fichier depuis le bucket Supabase\n",
    "        response = supabase.storage.from_(bucket_name).download(file_name) \n",
    "        # -> \"response\" contient les données brutes du fichier (bytes)\n",
    "\n",
    "        # Charger le CSV dans un DataFrame Pandas\n",
    "        df = pd.read_csv(io.BytesIO(response)) \n",
    "        # On convertit d'abord les bytes en un flux mémoire (io.BytesIO), \n",
    "        # puis Pandas lit le CSV et retourne un DataFrame\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Erreur de lecture du fichier depuis Supabase Storage :\")\n",
    "        print(\"->\", ex)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test de lecture \n",
    "df = read_csv_from_supabase(\"df_competence_rome_eda_v2.csv\")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Lecture du fichier réussi :\")\n",
    "    print(f\"-> CSV: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "    # df.shape renvoie (nb_lignes, nb_colonnes)\n",
    "\n",
    "    # Préparation des dictionnaires utiles\n",
    "\n",
    "    # Vocabulaire des compétences (chaque compétence unique reçoit un ID numérique)\n",
    "    skills_vocab = {s: i for i, s in enumerate(df['code_ogr_competence'].unique())}\n",
    "\n",
    "    # Vocabulaire des métiers (chaque code ROME reçoit un ID numérique)\n",
    "    jobs_vocab   = {j: i for i, j in enumerate(df['code_rome'].unique())}\n",
    "\n",
    "    # Dictionnaire : code ROME → libellé du métier\n",
    "    job_labels   = df.drop_duplicates('code_rome').set_index('code_rome')['libelle_rome'].to_dict()\n",
    "\n",
    "    # Dictionnaire : code ROME → liste des compétences associées\n",
    "    job_to_skills = df.groupby('code_rome')['code_ogr_competence'].apply(list).to_dict()\n",
    "\n",
    "    # Dictionnaire : code compétence → libellé compétence\n",
    "    skill_to_label = df.drop_duplicates('code_ogr_competence').set_index('code_ogr_competence')['libelle_competence'].to_dict()\n",
    "\n",
    "    # Statistiques\n",
    "    n_skills = len(skills_vocab)  # nombre total de compétences uniques\n",
    "    n_jobs = len(jobs_vocab)      # nombre total de métiers uniques\n",
    "\n",
    "    # Pondération simple (inspirée TF-IDF)\n",
    "    # Compter la fréquence de chaque compétence dans le DataFrame\n",
    "    skill_freq = df['code_ogr_competence'].value_counts().to_dict()\n",
    "\n",
    "    # Calculer un poids pour chaque compétence : \n",
    "    # plus une compétence est fréquente, plus son poids est faible\n",
    "    skill_weight = {s: 1.0 / math.log(1 + f) for s, f in skill_freq.items()}\n",
    "\n",
    "    print(f\"{n_skills} compétences et {n_jobs} métiers chargés depuis CSV\")\n",
    "\n",
    "else:\n",
    "    print(\"Impossible de charger le fichier.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Modèle Transformer\n",
    "# ---------------------------\n",
    "class JobProfileTransformer(nn.Module):\n",
    "    def __init__(self, n_skills, n_jobs, emb_dim=64, n_heads=4, n_layers=2, max_len=88):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding des compétences (chaque compétence reçoit un vecteur de dimension emb_dim)\n",
    "        self.skill_emb = nn.Embedding(n_skills, emb_dim)\n",
    "\n",
    "        # Encodage positionnel appris (pour donner une notion d’ordre dans la séquence)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, emb_dim))\n",
    "\n",
    "        # Définition d’une couche de Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,       # dimension des embeddings\n",
    "            nhead=n_heads,         # nombre de têtes d’attention\n",
    "            dim_feedforward=256,   # taille du réseau feedforward interne\n",
    "            batch_first=True       # batch en première dimension (batch, seq_len, emb_dim)\n",
    "        )\n",
    "\n",
    "        # Empilement de plusieurs couches d’encodeur\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Embedding des métiers (chaque métier reçoit aussi un vecteur)\n",
    "        self.job_emb = nn.Embedding(n_jobs, emb_dim)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Encode un \"profil de compétences\" en vecteur latent\n",
    "    # --------------------------------------------------------\n",
    "    def encode_profile(self, skills, weights=None):\n",
    "        batch_size, seq_len = skills.shape\n",
    "\n",
    "        # Gestion des séquences plus longues que max_len (on répète pos_emb si nécessaire)\n",
    "        if seq_len > self.pos_emb.size(1):\n",
    "            pos_emb = self.pos_emb.repeat(1, math.ceil(seq_len / self.pos_emb.size(1)), 1)[:, :seq_len, :]\n",
    "        else:\n",
    "            pos_emb = self.pos_emb[:, :seq_len, :]\n",
    "\n",
    "        # Embedding des compétences + ajout de l’encodage positionnel\n",
    "        skills_emb = self.skill_emb(skills) + pos_emb  \n",
    "\n",
    "        # Pondération optionnelle (par ex. TF-IDF pour valoriser les compétences rares)\n",
    "        if weights is not None:\n",
    "            skills_emb = skills_emb * weights.unsqueeze(-1)  \n",
    "\n",
    "        # Masque pour ignorer le padding (compétence codée par 0)\n",
    "        mask = (skills == 0)  \n",
    "\n",
    "        # Passage dans le Transformer Encoder\n",
    "        v = self.encoder(skills_emb, src_key_padding_mask=mask)\n",
    "\n",
    "        # Moyenne des vecteurs de la séquence pour obtenir une seule représentation\n",
    "        v = v.mean(dim=1)  \n",
    "\n",
    "        # Normalisation L2 → vecteur de norme 1 (utile pour la similarité cosinus)\n",
    "        return F.normalize(v, dim=1)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Encode un métier en vecteur latent\n",
    "    # --------------------------------------------------------\n",
    "    def encode_job(self, job_ids):\n",
    "        v = self.job_emb(job_ids)      # Embedding métier\n",
    "        return F.normalize(v, dim=1)   # Normalisation L2\n",
    "\n",
    "\n",
    "print(\"Modèle Transformer défini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cee677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Génération batch\n",
    "# ---------------------------\n",
    "def make_batch(batch_size=128, n_neg=20, max_mask_ratio=0.3, n_profiles_per_job=5):\n",
    "    # Listes pour stocker les données\n",
    "    skills_batch, weights_batch, pos_jobs_batch, neg_jobs_batch = [], [], [], []\n",
    "\n",
    "    # Liste de tous les métiers disponibles (clés du vocabulaire)\n",
    "    jobs_list = list(jobs_vocab.keys())\n",
    "\n",
    "    # Génération de batch_size exemples\n",
    "    for _ in range(batch_size):\n",
    "        job = random.choice(jobs_list)                 # Tirage aléatoire d’un métier\n",
    "        comps = job_to_skills[job]                     # Liste des compétences associées à ce métier\n",
    "\n",
    "        # Générer plusieurs \"profils\" pour ce métier\n",
    "        for _ in range(n_profiles_per_job):\n",
    "            # Masquage partiel des compétences\n",
    "            n_mask = random.randint(0, int(len(comps) * max_mask_ratio)) # Nb de compétences à masquer\n",
    "            masked = random.sample(comps, k=max(1, len(comps) - n_mask)) # Séquence de compétences gardées\n",
    "\n",
    "            # Ajout des compétences encodées en indices\n",
    "            skills_batch.append([skills_vocab[c] for c in masked])\n",
    "\n",
    "            # Ajout des poids associés (ex. pondération TF-IDF)\n",
    "            weights_batch.append([skill_weight[c] for c in masked])\n",
    "\n",
    "            # Label positif = le métier d’origine\n",
    "            pos_jobs_batch.append(jobs_vocab[job])\n",
    "\n",
    "            # Tirage de métiers négatifs (métiers différents du métier choisi)\n",
    "            negs = random.sample([j for j in jobs_list if j != job], n_neg)\n",
    "            neg_jobs_batch.append([jobs_vocab[j] for j in negs])\n",
    "\n",
    "    # Déterminer la longueur maximale de séquence de compétences\n",
    "    max_len = max(len(s) for s in skills_batch)\n",
    "\n",
    "    # Padding pour que toutes les séquences aient la même longueur\n",
    "    for i in range(len(skills_batch)):\n",
    "        while len(skills_batch[i]) < max_len:\n",
    "            skills_batch[i].append(0)     # 0 = token \"vide\"\n",
    "            weights_batch[i].append(0.0)  # poids nul\n",
    "\n",
    "    # --- Conversion en tenseurs PyTorch\n",
    "    return (\n",
    "        torch.tensor(skills_batch), \n",
    "        torch.tensor(weights_batch, dtype=torch.float), \n",
    "        torch.tensor(pos_jobs_batch), \n",
    "        torch.tensor(neg_jobs_batch)\n",
    "    ) \n",
    "\n",
    "print(\"Fonction de génération de batch définie\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b97922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Loss contrastive\n",
    "# ---------------------------\n",
    "def contrastive_ranking_loss(v_p, v_j_pos, v_j_neg, temperature=0.1):\n",
    "    # v_p      : vecteurs profils (batch_size, emb_dim)\n",
    "    # v_j_pos  : vecteurs métiers positifs (batch_size, emb_dim)\n",
    "    # v_j_neg  : vecteurs métiers négatifs (batch_size, n_neg, emb_dim)\n",
    "    # temperature : facteur d’échelle pour adoucir ou accentuer les différences\n",
    "\n",
    "    # Similarité profil / métier positif \n",
    "    # Produit scalaire entre le profil et son vrai métier\n",
    "    pos_sim = torch.sum(v_p * v_j_pos, dim=1, keepdim=True)  \n",
    "    # shape : (batch_size, 1)\n",
    "\n",
    "    # Similarité profil / métiers négatifs\n",
    "    # Produit matriciel entre chaque métier négatif et le profil\n",
    "    neg_sim = torch.bmm(v_j_neg, v_p.unsqueeze(-1)).squeeze(-1)  \n",
    "    # shape : (batch_size, n_neg)\n",
    "\n",
    "    # Concaténation des scores (positif + négatifs)\n",
    "    logits = torch.cat([pos_sim, neg_sim], dim=1) / temperature  \n",
    "    # shape : (batch_size, 1 + n_neg)\n",
    "\n",
    "    # Labels : 0 correspond au vrai métier (position du positif)\n",
    "    labels = torch.zeros(v_p.size(0), dtype=torch.long, device=v_p.device)\n",
    "\n",
    "    # Perte de classification (cross-entropy)\n",
    "    # Le modèle apprend à classer \"0\" (le vrai métier) comme le plus probable\n",
    "    return F.cross_entropy(logits, labels)  \n",
    "\n",
    "print(\"Fonction de perte définie\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Entraînement et sauvegarde sur le bucket Supabase (Attention : cela écrase le modèle existant si le nom est identique)\n",
    "# ---------------------------\n",
    "\n",
    "# Choix du device (GPU si dispo sinon CPU) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialisation du modèle et de l’optimiseur ---\n",
    "model = JobProfileTransformer(n_skills, n_jobs, emb_dim=64).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Nombre d’époques d’entraînement \n",
    "n_epochs = 10   # (test rapide, ton vrai entraînement était sur 4000 epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # --- Génération d’un batch aléatoire \n",
    "    skills_batch, weights_batch, pos_jobs, neg_jobs = make_batch(\n",
    "        batch_size=128,   # nombre d’échantillons par batch\n",
    "        n_neg=20,         # métiers négatifs par profil\n",
    "        n_profiles_per_job=5\n",
    "    )\n",
    "\n",
    "    # Envoi des données sur GPU/CPU\n",
    "    skills_batch, weights_batch = skills_batch.to(device), weights_batch.to(device)\n",
    "    pos_jobs, neg_jobs = pos_jobs.to(device), neg_jobs.to(device)\n",
    "\n",
    "    # Encodage profil et métiers\n",
    "    v_p = model.encode_profile(skills_batch, weights_batch)     # (batch_size, emb_dim)\n",
    "    v_j_pos = model.encode_job(pos_jobs)                        # vrai métier\n",
    "    v_j_neg = model.encode_job(neg_jobs).view(v_p.size(0), 20, -1)  # métiers négatifs\n",
    "\n",
    "    # Calcul de la loss contrastive \n",
    "    loss = contrastive_ranking_loss(v_p, v_j_pos, v_j_neg)\n",
    "\n",
    "    # Backpropagation \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Affichage (ici tous les 100 epochs, mais tu es sur 10 seulement)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Sauvegarde du modèle\n",
    "# ---------------------------\n",
    "\n",
    "# Sauvegarde du modèle dans un buffer en mémoire\n",
    "buffer = io.BytesIO()\n",
    "torch.save(model.state_dict(), buffer)   # on sauvegarde les poids (state_dict)\n",
    "buffer.seek(0)\n",
    "model_bytes = buffer.getvalue()          # conversion en bytes pour upload\n",
    "\n",
    "# Nom du fichier sauvegardé dans Supabase\n",
    "name_file = 'job_transformer_test.pth'  #  changer si tu veux éviter l’écrasement\n",
    "\n",
    "# Vérification si un fichier du même nom existe déjà\n",
    "existing_files = supabase.storage.from_('DL').list()\n",
    "if any(f['name'] == name_file for f in existing_files):\n",
    "    # Suppression de l’ancien fichier\n",
    "    supabase.storage.from_('dlhybride').remove([name_file])\n",
    "\n",
    "# Upload du nouveau modèle dans Supabase (upsert = écrase si existe déjà)\n",
    "supabase.storage.from_('dlhybride').upload(\n",
    "    name_file, \n",
    "    model_bytes,\n",
    "    {'content-type': 'application/octet-stream', 'upsert': 'true'}\n",
    ")\n",
    "\n",
    "print(\"Modèle sauvegardé dans le bucket DL de Supabase\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Résultats d’entraînement\n",
    "# ---------------------------\n",
    "#Epoch 1000, loss=0.2929 22min 52sec\n",
    "#Epoch 1500, loss=0.1680 28min 27sec\n",
    "#Epoch 2000, loss=0.1226 34min 10sec\n",
    "#Epoch 2500, loss=0.0817 50min 16sec\n",
    "#Epoch 3000, loss=0.0707 59min 01sec\n",
    "#Epoch 3500, loss=0.0492 59min 41sec\n",
    "#Epoch 4000, loss=0.0465 69min 11sec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5356af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 6. Rechargement du modèle\n",
    "# ---------------------------\n",
    "\n",
    "# Paramètres du bucket et du fichier à télécharger\n",
    "bucket_name = SUPABASE_BUCKET\n",
    "file_name = \"job_transformer_4000.pth\"  #  changer le nom selon le modèle que tu veux recharger\n",
    "\n",
    "# Téléchargement du modèle depuis Supabase\n",
    "res = supabase.storage.from_(bucket_name).download(file_name)\n",
    "\n",
    "# Vérification : si le fichier est vide ou absent → erreur\n",
    "if res is None or len(res) == 0:\n",
    "    raise Exception(\"Erreur téléchargement : fichier non trouvé ou vide\")\n",
    "\n",
    "# Écriture temporaire du fichier en local\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "    tmp_file.write(res)                 # on écrit les bytes téléchargés\n",
    "    tmp_model_path = tmp_file.name      # chemin du fichier temporaire\n",
    "\n",
    "# Création d’un modèle identique à l’original\n",
    "# il faut utiliser la même définition (n_skills, n_jobs, emb_dim)\n",
    "model_loaded = JobProfileTransformer(n_skills, n_jobs, emb_dim=64).to(device)\n",
    "\n",
    "# Chargement des poids sauvegardés\n",
    "model_loaded.load_state_dict(torch.load(tmp_model_path, map_location=device))\n",
    "\n",
    "# Passage en mode évaluation (désactive dropout, batchnorm, etc.)\n",
    "model_loaded.eval()\n",
    "\n",
    "print(\"Modèle rechargé et prêt pour simulation \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7. Création de 25 faux profils + upload Supabase\n",
    "# ---------------------------\n",
    "\n",
    "# Construction des mappings métiers → compétences et labels\n",
    "job_to_skills = df.groupby(\"code_rome\")[\"code_ogr_competence\"].apply(list).to_dict()\n",
    "job_labels = df.drop_duplicates(\"code_rome\").set_index(\"code_rome\")[\"libelle_rome\"].to_dict()\n",
    "skill_labels = df.drop_duplicates(\"code_ogr_competence\").set_index(\"code_ogr_competence\")[\"libelle_competence\"].to_dict()\n",
    "\n",
    "# Fonction pour générer un faux profil à partir d'un métier\n",
    "def generate_fake_profile(job_code, min_ratio=0.5, max_ratio=0.9): \n",
    "    \"\"\"\n",
    "    Prend un métier (job_code) et retourne une sous-liste de compétences.\n",
    "    min_ratio/max_ratio : proportion de compétences à garder\n",
    "    \"\"\"\n",
    "    skills = job_to_skills[job_code]                  # compétences du métier\n",
    "    keep_ratio = random.uniform(min_ratio, max_ratio) # ratio aléatoire\n",
    "    n_keep = max(1, int(len(skills) * keep_ratio))    # nombre de compétences gardées\n",
    "    selected = random.sample(skills, n_keep)          # tirage aléatoire\n",
    "    return selected\n",
    "\n",
    "# Générer 25 faux profils\n",
    "fake_profiles = []\n",
    "for _ in range(25):\n",
    "    job = random.choice(list(job_to_skills.keys()))  # métier aléatoire\n",
    "    selected_skills = generate_fake_profile(job)\n",
    "    fake_profiles.append({\n",
    "        \"job_code\": job,\n",
    "        \"job_label\": job_labels.get(job, \"?\"),\n",
    "        \"skills\": selected_skills,\n",
    "        \"skills_labels\": [skill_labels.get(s, \"?\") for s in selected_skills]\n",
    "    })\n",
    "\n",
    "# Transformation en format tableau pour CSV\n",
    "rows = []\n",
    "for i, profile in enumerate(fake_profiles, 1):\n",
    "    for skill, label in zip(profile[\"skills\"], profile[\"skills_labels\"]):\n",
    "        rows.append({\n",
    "            \"profile_id\": i,\n",
    "            \"job_code\": profile[\"job_code\"],\n",
    "            \"job_label\": profile[\"job_label\"],\n",
    "            \"skill_code\": skill,\n",
    "            \"skill_label\": label\n",
    "        })\n",
    "\n",
    "df_profiles = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------------------\n",
    "# Upload vers Supabase (écrasement autorisé)\n",
    "# ---------------------------\n",
    "def upload_df_to_supabase(df, file_name=\"fake_profiles.csv\", bucket_name=SUPABASE_BUCKET):\n",
    "    try:\n",
    "        # Sauvegarde en mémoire\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False, encoding=\"utf-8\")\n",
    "        file_bytes = csv_buffer.getvalue().encode(\"utf-8\")\n",
    "\n",
    "        # Upload dans le bucket Supabase\n",
    "        supabase.storage.from_(bucket_name).upload(\n",
    "            path=file_name,\n",
    "            file=file_bytes,\n",
    "            file_options={\"content-type\": \"text/csv\", \"upsert\": \"true\"}  # upsert = écrase si existant\n",
    "        )\n",
    "\n",
    "        print(f\"Fichier '{file_name}' uploadé (remplacé si déjà existant) dans le bucket '{bucket_name}'\")\n",
    "    except Exception as ex:\n",
    "        print(\"Erreur lors de l'upload :\")\n",
    "        print(\"->\", ex)\n",
    "\n",
    "# Exécution de l’upload\n",
    "upload_df_to_supabase(df_profiles, \"fake_profiles.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 8. Charger un CSV depuis Supabase\n",
    "# ---------------------------\n",
    "def load_csv_from_supabase(file_name, bucket_name=\"dlhybride\"):\n",
    "    try:\n",
    "        files = supabase.storage.from_(bucket_name).list()\n",
    "    \n",
    "        response = supabase.storage.from_(bucket_name).download(file_name)\n",
    "        if response is None or len(response) == 0:\n",
    "            print(f\"Le fichier '{file_name}' est vide ou inexistant.\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_csv(io.BytesIO(response), dtype=str)\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {file_name} depuis Supabase : {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Charger df_competence_rome_eda_v2.csv\n",
    "# ---------------------------\n",
    "df_jobs = load_csv_from_supabase(\"df_competence_rome_eda_v2.csv\")\n",
    "if df_jobs is None:\n",
    "    raise FileNotFoundError(\"Impossible de charger df_competence_rome_eda_v2.csv\")\n",
    "\n",
    "df_jobs['code_ogr_competence'] = df_jobs['code_ogr_competence'].astype(str)\n",
    "\n",
    "# Mappings compétences\n",
    "skills_vocab = {code: idx for idx, code in enumerate(df_jobs['code_ogr_competence'].unique())}\n",
    "skill_to_label = df_jobs.drop_duplicates('code_ogr_competence') \\\n",
    "                        .set_index('code_ogr_competence')['libelle_competence'].to_dict()\n",
    "\n",
    "# Mappings métiers\n",
    "jobs_vocab = {rome: idx for idx, rome in enumerate(df_jobs['code_rome'].unique())}\n",
    "job_labels = df_jobs.drop_duplicates('code_rome').set_index('code_rome')['libelle_rome'].to_dict()\n",
    "job_to_skills = df_jobs.groupby('code_rome')['code_ogr_competence'].apply(set).to_dict()\n",
    "\n",
    "# ---------------------------\n",
    "# Fonction predict_hybrid \n",
    "# ---------------------------\n",
    "def predict_hybrid(model, input_skills, skills_vocab, job_to_skills, jobs_vocab, job_labels,\n",
    "                   top_k=3, seuil=0.3, min_overlap=2):\n",
    "    device = next(model.parameters()).device\n",
    "    ids = [skills_vocab[s] for s in input_skills if s in skills_vocab]\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        return \"Indéfini (aucune compétence reconnue)\"\n",
    "\n",
    "    skills = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "    weights = torch.tensor([1.0 for _ in ids], dtype=torch.float).unsqueeze(0).to(device)\n",
    "    \n",
    "    v_p = model.encode_profile(skills, weights)\n",
    "    all_jobs = torch.arange(len(jobs_vocab)).to(device)\n",
    "    v_j = model.encode_job(all_jobs)\n",
    "    \n",
    "    scores_dl = (v_p @ v_j.T).squeeze(0)\n",
    "    \n",
    "    input_set = set(input_skills)\n",
    "    overlap_scores_list = [len(input_set & set(job_to_skills.get(j, []))) for j in jobs_vocab.keys()]\n",
    "    \n",
    "    if len(overlap_scores_list) == 0:\n",
    "        combined_scores = scores_dl\n",
    "    else:\n",
    "        overlap_scores = torch.tensor(overlap_scores_list, device=device)\n",
    "        combined_scores = 0.3 * scores_dl + 0.7 * (overlap_scores / max(1, max(overlap_scores)))\n",
    "\n",
    "    mask = overlap_scores_list and overlap_scores >= min_overlap if len(overlap_scores_list) > 0 else torch.ones_like(scores_dl, dtype=torch.bool)\n",
    "    filtered_indices = torch.arange(len(jobs_vocab), device=device)[mask]\n",
    "    filtered_scores = combined_scores[mask]\n",
    "    \n",
    "    if len(filtered_scores) == 0:\n",
    "        return \"Indéfini (aucune compétence ne passe le filtre)\"\n",
    "\n",
    "    best_scores, best_idx = filtered_scores.topk(min(top_k, len(filtered_scores)))\n",
    "    best_jobs = [list(jobs_vocab.keys())[i] for i in filtered_indices[best_idx]]\n",
    "\n",
    "    lines = []\n",
    "    for rome, s in zip(best_jobs, best_scores):\n",
    "        libelle = job_labels.get(rome, \"?\")\n",
    "        # detach() pour éviter le warning PyTorch\n",
    "        lines.append(f\"{rome} - {libelle} - {round(float(s.detach().cpu())*100,1)}%\")\n",
    "    \n",
    "    if best_scores[0] < seuil:\n",
    "        return \"Indéfini\\n\" + \"\\n\".join(lines)\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 9. Charger et nettoyer les profils\n",
    "# ---------------------------\n",
    "def load_fake_profiles_from_supabase(file_name=\"fake_profiles.csv\", bucket_name=SUPABASE_BUCKET):\n",
    "    if bucket_name is None or len(bucket_name) < 3:\n",
    "        bucket_name = \"dlhybride\"\n",
    "        print(f\"Bucket name trop court ou absent, utilisation de '{bucket_name}'\")\n",
    "    try:\n",
    "        files = supabase.storage.from_(bucket_name).list()\n",
    "        \n",
    "        response = supabase.storage.from_(bucket_name).download(file_name)\n",
    "        if response is None or len(response) == 0:\n",
    "            print(f\"Le fichier '{file_name}' n'existe pas ou est vide dans le bucket '{bucket_name}'\")\n",
    "            return None\n",
    "        df = pd.read_csv(io.BytesIO(response), dtype=str)\n",
    "        df['skill_code'] = df['skill_code'].str.strip()  # Supprimer espaces éventuels\n",
    "        return df\n",
    "    except Exception as ex:\n",
    "        print(\"Erreur lors de la lecture depuis Supabase :\", ex)\n",
    "        return None\n",
    "\n",
    "fake_profiles = load_fake_profiles_from_supabase()\n",
    "\n",
    "# ---------------------------\n",
    "# Vérifier compétences reconnues\n",
    "# ---------------------------\n",
    "skills_vocab_clean = {k.strip(): v for k,v in skills_vocab.items()}  # enlever espaces\n",
    "df_jobs['code_ogr_competence'] = df_jobs['code_ogr_competence'].str.strip()\n",
    "\n",
    "# Mapping code -> label\n",
    "skill_to_label = df_jobs.drop_duplicates('code_ogr_competence') \\\n",
    "                        .set_index('code_ogr_competence')['libelle_competence'].to_dict()\n",
    "\n",
    "# ---------------------------\n",
    "# Boucle de prédiction par profil avec comparaison au métier attendu\n",
    "# ---------------------------\n",
    "for profile_id in fake_profiles['profile_id'].unique():\n",
    "    subset = fake_profiles[fake_profiles['profile_id'] == profile_id]\n",
    "    user_skills = subset['skill_code'].tolist()\n",
    "\n",
    "    # Compétence attendue (colonne à adapter si elle s'appelle différemment dans ton CSV)\n",
    "    expected_job = subset['job_code'].iloc[0] if 'job_code' in subset.columns else None\n",
    "\n",
    "    # Identifier compétences reconnues et non reconnues\n",
    "    recognized_skills = [s for s in user_skills if s in skills_vocab_clean]\n",
    "    unrecognized_skills = [s for s in user_skills if s not in skills_vocab_clean]\n",
    "\n",
    "    user_skills_named = [f\"{c} - {skill_to_label.get(c,'?')}\" for c in recognized_skills]\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Profil {profile_id} → compétences ({len(user_skills)}):\")\n",
    "    for s in user_skills_named:\n",
    "        print(f\"   • {s}\")\n",
    "    if unrecognized_skills:\n",
    "        print(f\"Compétences non reconnues ({len(unrecognized_skills)}): {unrecognized_skills}\")\n",
    "\n",
    "    if expected_job:\n",
    "        print(f\"\\nMétier attendu : {expected_job} - {job_labels.get(expected_job,'?')}\")\n",
    "\n",
    "    print(\"\\nTop-3 métiers proposés :\")\n",
    "    try:\n",
    "        prediction = predict_hybrid(\n",
    "            model, recognized_skills,\n",
    "            skills_vocab_clean, job_to_skills, jobs_vocab, job_labels,\n",
    "            top_k=3\n",
    "        )\n",
    "        print(prediction)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la prédiction : {e}\")\n",
    "\n",
    "    print(\"\\nPrédictions terminées\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
